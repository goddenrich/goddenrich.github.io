<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=wMXA-lZiHBMEZ-dBsKceyg');.lst-kix_1gm9o9wcd7pg-0>li:before{content:"" counter(lst-ctn-kix_1gm9o9wcd7pg-0,decimal) ". "}.lst-kix_f0cuw8wlviqi-7>li{counter-increment:lst-ctn-kix_f0cuw8wlviqi-7}.lst-kix_6nqn63a3krj0-7>li{counter-increment:lst-ctn-kix_6nqn63a3krj0-7}ol.lst-kix_a9i1uwuflx9z-8.start{counter-reset:lst-ctn-kix_a9i1uwuflx9z-8 0}ol.lst-kix_6wajunywya39-6{list-style-type:none}.lst-kix_1gm9o9wcd7pg-1>li:before{content:"" counter(lst-ctn-kix_1gm9o9wcd7pg-1,lower-latin) ". "}.lst-kix_1gm9o9wcd7pg-2>li:before{content:"" counter(lst-ctn-kix_1gm9o9wcd7pg-2,lower-roman) ". "}ol.lst-kix_6wajunywya39-7{list-style-type:none}ol.lst-kix_6nqn63a3krj0-8.start{counter-reset:lst-ctn-kix_6nqn63a3krj0-8 0}ol.lst-kix_6wajunywya39-4{list-style-type:none}ol.lst-kix_f0cuw8wlviqi-0.start{counter-reset:lst-ctn-kix_f0cuw8wlviqi-0 0}ol.lst-kix_6wajunywya39-5{list-style-type:none}ol.lst-kix_6wajunywya39-2{list-style-type:none}.lst-kix_6wajunywya39-7>li{counter-increment:lst-ctn-kix_6wajunywya39-7}ol.lst-kix_6wajunywya39-3{list-style-type:none}ol.lst-kix_6wajunywya39-0{list-style-type:none}.lst-kix_ex0q9wj3jmqv-0>li{counter-increment:lst-ctn-kix_ex0q9wj3jmqv-0}ol.lst-kix_6wajunywya39-1{list-style-type:none}ol.lst-kix_k49x0wlmrunk-6.start{counter-reset:lst-ctn-kix_k49x0wlmrunk-6 0}ol.lst-kix_1gm9o9wcd7pg-2.start{counter-reset:lst-ctn-kix_1gm9o9wcd7pg-2 0}ol.lst-kix_6wajunywya39-8{list-style-type:none}.lst-kix_k49x0wlmrunk-3>li{counter-increment:lst-ctn-kix_k49x0wlmrunk-3}ol.lst-kix_ex0q9wj3jmqv-0.start{counter-reset:lst-ctn-kix_ex0q9wj3jmqv-0 0}ol.lst-kix_list_1-5.start{counter-reset:lst-ctn-kix_list_1-5 0}.lst-kix_1gm9o9wcd7pg-3>li{counter-increment:lst-ctn-kix_1gm9o9wcd7pg-3}ol.lst-kix_a9i1uwuflx9z-2.start{counter-reset:lst-ctn-kix_a9i1uwuflx9z-2 0}ol.lst-kix_1gm9o9wcd7pg-7.start{counter-reset:lst-ctn-kix_1gm9o9wcd7pg-7 0}ol.lst-kix_6nqn63a3krj0-2.start{counter-reset:lst-ctn-kix_6nqn63a3krj0-2 0}ol.lst-kix_6wajunywya39-5.start{counter-reset:lst-ctn-kix_6wajunywya39-5 0}.lst-kix_f0cuw8wlviqi-7>li:before{content:"" counter(lst-ctn-kix_f0cuw8wlviqi-7,lower-latin) ". "}.lst-kix_list_1-2>li{counter-increment:lst-ctn-kix_list_1-2}.lst-kix_f0cuw8wlviqi-6>li:before{content:"" counter(lst-ctn-kix_f0cuw8wlviqi-6,decimal) ". "}.lst-kix_f0cuw8wlviqi-8>li:before{content:"" counter(lst-ctn-kix_f0cuw8wlviqi-8,lower-roman) ". "}.lst-kix_ex0q9wj3jmqv-2>li{counter-increment:lst-ctn-kix_ex0q9wj3jmqv-2}.lst-kix_6nqn63a3krj0-5>li{counter-increment:lst-ctn-kix_6nqn63a3krj0-5}.lst-kix_f0cuw8wlviqi-0>li:before{content:"" counter(lst-ctn-kix_f0cuw8wlviqi-0,decimal) ". "}ol.lst-kix_a9i1uwuflx9z-3.start{counter-reset:lst-ctn-kix_a9i1uwuflx9z-3 0}.lst-kix_list_1-4>li{counter-increment:lst-ctn-kix_list_1-4}ol.lst-kix_list_1-6.start{counter-reset:lst-ctn-kix_list_1-6 0}.lst-kix_f0cuw8wlviqi-3>li:before{content:"" counter(lst-ctn-kix_f0cuw8wlviqi-3,decimal) ". "}ol.lst-kix_1gm9o9wcd7pg-1.start{counter-reset:lst-ctn-kix_1gm9o9wcd7pg-1 0}.lst-kix_f0cuw8wlviqi-4>li:before{content:"" counter(lst-ctn-kix_f0cuw8wlviqi-4,lower-latin) ". "}.lst-kix_f0cuw8wlviqi-5>li:before{content:"" counter(lst-ctn-kix_f0cuw8wlviqi-5,lower-roman) ". "}ol.lst-kix_f0cuw8wlviqi-5.start{counter-reset:lst-ctn-kix_f0cuw8wlviqi-5 0}ol.lst-kix_6wajunywya39-0.start{counter-reset:lst-ctn-kix_6wajunywya39-0 0}ol.lst-kix_ex0q9wj3jmqv-5.start{counter-reset:lst-ctn-kix_ex0q9wj3jmqv-5 0}.lst-kix_f0cuw8wlviqi-2>li:before{content:"" counter(lst-ctn-kix_f0cuw8wlviqi-2,lower-roman) ". "}ol.lst-kix_k49x0wlmrunk-0.start{counter-reset:lst-ctn-kix_k49x0wlmrunk-0 0}.lst-kix_6wajunywya39-3>li{counter-increment:lst-ctn-kix_6wajunywya39-3}.lst-kix_f0cuw8wlviqi-1>li:before{content:"" counter(lst-ctn-kix_f0cuw8wlviqi-1,lower-latin) ". "}.lst-kix_f0cuw8wlviqi-5>li{counter-increment:lst-ctn-kix_f0cuw8wlviqi-5}.lst-kix_ex0q9wj3jmqv-4>li{counter-increment:lst-ctn-kix_ex0q9wj3jmqv-4}.lst-kix_6nqn63a3krj0-8>li:before{content:"" counter(lst-ctn-kix_6nqn63a3krj0-8,lower-roman) ". "}ol.lst-kix_list_1-0.start{counter-reset:lst-ctn-kix_list_1-0 0}.lst-kix_6nqn63a3krj0-6>li:before{content:"" counter(lst-ctn-kix_6nqn63a3krj0-6,decimal) ". "}.lst-kix_list_3-0>li{counter-increment:lst-ctn-kix_list_3-0}.lst-kix_a9i1uwuflx9z-0>li{counter-increment:lst-ctn-kix_a9i1uwuflx9z-0}ol.lst-kix_list_4-0.start{counter-reset:lst-ctn-kix_list_4-0 0}ol.lst-kix_6nqn63a3krj0-7.start{counter-reset:lst-ctn-kix_6nqn63a3krj0-7 0}.lst-kix_6nqn63a3krj0-7>li:before{content:"" counter(lst-ctn-kix_6nqn63a3krj0-7,lower-latin) ". "}.lst-kix_1gm9o9wcd7pg-5>li:before{content:"" counter(lst-ctn-kix_1gm9o9wcd7pg-5,lower-roman) ". "}.lst-kix_1gm9o9wcd7pg-6>li:before{content:"" counter(lst-ctn-kix_1gm9o9wcd7pg-6,decimal) ". "}ol.lst-kix_1gm9o9wcd7pg-8.start{counter-reset:lst-ctn-kix_1gm9o9wcd7pg-8 0}ol.lst-kix_k49x0wlmrunk-1.start{counter-reset:lst-ctn-kix_k49x0wlmrunk-1 0}.lst-kix_1gm9o9wcd7pg-3>li:before{content:"" counter(lst-ctn-kix_1gm9o9wcd7pg-3,decimal) ". "}.lst-kix_1gm9o9wcd7pg-4>li:before{content:"" counter(lst-ctn-kix_1gm9o9wcd7pg-4,lower-latin) ". "}.lst-kix_1gm9o9wcd7pg-7>li:before{content:"" counter(lst-ctn-kix_1gm9o9wcd7pg-7,lower-latin) ". "}.lst-kix_1gm9o9wcd7pg-8>li:before{content:"" counter(lst-ctn-kix_1gm9o9wcd7pg-8,lower-roman) ". "}ol.lst-kix_f0cuw8wlviqi-6.start{counter-reset:lst-ctn-kix_f0cuw8wlviqi-6 0}ol.lst-kix_k49x0wlmrunk-7.start{counter-reset:lst-ctn-kix_k49x0wlmrunk-7 0}.lst-kix_1gm9o9wcd7pg-7>li{counter-increment:lst-ctn-kix_1gm9o9wcd7pg-7}ol.lst-kix_ex0q9wj3jmqv-6.start{counter-reset:lst-ctn-kix_ex0q9wj3jmqv-6 0}.lst-kix_1gm9o9wcd7pg-1>li{counter-increment:lst-ctn-kix_1gm9o9wcd7pg-1}.lst-kix_k49x0wlmrunk-5>li{counter-increment:lst-ctn-kix_k49x0wlmrunk-5}ol.lst-kix_list_1-3{list-style-type:none}.lst-kix_ex0q9wj3jmqv-7>li:before{content:"" counter(lst-ctn-kix_ex0q9wj3jmqv-7,lower-latin) ". "}ol.lst-kix_list_1-4{list-style-type:none}.lst-kix_list_2-7>li:before{content:" "}.lst-kix_v16i7znt6euc-2>li:before{content:"-  "}ol.lst-kix_list_1-5{list-style-type:none}ol.lst-kix_list_1-6{list-style-type:none}ol.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_2-5>li:before{content:" "}.lst-kix_ex0q9wj3jmqv-6>li{counter-increment:lst-ctn-kix_ex0q9wj3jmqv-6}.lst-kix_v16i7znt6euc-0>li:before{content:"-  "}ol.lst-kix_list_1-1{list-style-type:none}.lst-kix_v16i7znt6euc-4>li:before{content:"-  "}.lst-kix_k49x0wlmrunk-4>li:before{content:"" counter(lst-ctn-kix_k49x0wlmrunk-4,lower-latin) ". "}.lst-kix_k49x0wlmrunk-8>li:before{content:"" counter(lst-ctn-kix_k49x0wlmrunk-8,lower-roman) ". "}ol.lst-kix_list_1-2{list-style-type:none}ol.lst-kix_a9i1uwuflx9z-1.start{counter-reset:lst-ctn-kix_a9i1uwuflx9z-1 0}ol.lst-kix_1gm9o9wcd7pg-6.start{counter-reset:lst-ctn-kix_1gm9o9wcd7pg-6 0}.lst-kix_ex0q9wj3jmqv-5>li:before{content:"" counter(lst-ctn-kix_ex0q9wj3jmqv-5,lower-roman) ". "}.lst-kix_6nqn63a3krj0-1>li{counter-increment:lst-ctn-kix_6nqn63a3krj0-1}.lst-kix_k49x0wlmrunk-6>li:before{content:"" counter(lst-ctn-kix_k49x0wlmrunk-6,decimal) ". "}ol.lst-kix_6wajunywya39-6.start{counter-reset:lst-ctn-kix_6wajunywya39-6 0}.lst-kix_f0cuw8wlviqi-2>li{counter-increment:lst-ctn-kix_f0cuw8wlviqi-2}.lst-kix_6nqn63a3krj0-0>li:before{content:"" counter(lst-ctn-kix_6nqn63a3krj0-0,decimal) ". "}ul.lst-kix_list_3-7{list-style-type:none}ul.lst-kix_list_3-8{list-style-type:none}.lst-kix_ex0q9wj3jmqv-1>li:before{content:"" counter(lst-ctn-kix_ex0q9wj3jmqv-1,lower-latin) ". "}.lst-kix_6nqn63a3krj0-2>li:before{content:"" counter(lst-ctn-kix_6nqn63a3krj0-2,lower-roman) ". "}.lst-kix_v16i7znt6euc-8>li:before{content:"-  "}ol.lst-kix_list_3-0.start{counter-reset:lst-ctn-kix_list_3-0 0}ul.lst-kix_list_3-1{list-style-type:none}.lst-kix_6nqn63a3krj0-4>li:before{content:"" counter(lst-ctn-kix_6nqn63a3krj0-4,lower-latin) ". "}ul.lst-kix_list_3-2{list-style-type:none}.lst-kix_ex0q9wj3jmqv-3>li:before{content:"" counter(lst-ctn-kix_ex0q9wj3jmqv-3,decimal) ". "}ol.lst-kix_f0cuw8wlviqi-7.start{counter-reset:lst-ctn-kix_f0cuw8wlviqi-7 0}ol.lst-kix_list_1-7{list-style-type:none}ul.lst-kix_list_3-5{list-style-type:none}.lst-kix_6nqn63a3krj0-0>li{counter-increment:lst-ctn-kix_6nqn63a3krj0-0}ol.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_3-6{list-style-type:none}ul.lst-kix_list_3-3{list-style-type:none}.lst-kix_v16i7znt6euc-6>li:before{content:"-  "}ul.lst-kix_list_3-4{list-style-type:none}ol.lst-kix_6wajunywya39-3.start{counter-reset:lst-ctn-kix_6wajunywya39-3 0}.lst-kix_a9i1uwuflx9z-7>li:before{content:"" counter(lst-ctn-kix_a9i1uwuflx9z-7,lower-latin) ". "}.lst-kix_1gm9o9wcd7pg-6>li{counter-increment:lst-ctn-kix_1gm9o9wcd7pg-6}.lst-kix_a9i1uwuflx9z-5>li:before{content:"(" counter(lst-ctn-kix_a9i1uwuflx9z-5,lower-roman) ") "}.lst-kix_ex0q9wj3jmqv-7>li{counter-increment:lst-ctn-kix_ex0q9wj3jmqv-7}.lst-kix_list_4-1>li:before{content:" "}.lst-kix_ex0q9wj3jmqv-5>li{counter-increment:lst-ctn-kix_ex0q9wj3jmqv-5}.lst-kix_list_4-3>li:before{content:" "}.lst-kix_list_4-5>li:before{content:" "}ol.lst-kix_1gm9o9wcd7pg-3.start{counter-reset:lst-ctn-kix_1gm9o9wcd7pg-3 0}.lst-kix_a9i1uwuflx9z-1>li:before{content:"" counter(lst-ctn-kix_a9i1uwuflx9z-1,lower-latin) ") "}ol.lst-kix_ex0q9wj3jmqv-4.start{counter-reset:lst-ctn-kix_ex0q9wj3jmqv-4 0}.lst-kix_list_1-8>li{counter-increment:lst-ctn-kix_list_1-8}.lst-kix_a9i1uwuflx9z-7>li{counter-increment:lst-ctn-kix_a9i1uwuflx9z-7}.lst-kix_a9i1uwuflx9z-3>li:before{content:"(" counter(lst-ctn-kix_a9i1uwuflx9z-3,decimal) ") "}ol.lst-kix_list_1-4.start{counter-reset:lst-ctn-kix_list_1-4 0}.lst-kix_6wajunywya39-1>li{counter-increment:lst-ctn-kix_6wajunywya39-1}ol.lst-kix_list_1-1.start{counter-reset:lst-ctn-kix_list_1-1 0}ol.lst-kix_ex0q9wj3jmqv-1.start{counter-reset:lst-ctn-kix_ex0q9wj3jmqv-1 0}ol.lst-kix_6wajunywya39-4.start{counter-reset:lst-ctn-kix_6wajunywya39-4 0}ol.lst-kix_ex0q9wj3jmqv-2.start{counter-reset:lst-ctn-kix_ex0q9wj3jmqv-2 0}.lst-kix_a9i1uwuflx9z-2>li{counter-increment:lst-ctn-kix_a9i1uwuflx9z-2}.lst-kix_a9i1uwuflx9z-8>li{counter-increment:lst-ctn-kix_a9i1uwuflx9z-8}.lst-kix_1gm9o9wcd7pg-5>li{counter-increment:lst-ctn-kix_1gm9o9wcd7pg-5}ol.lst-kix_6nqn63a3krj0-3.start{counter-reset:lst-ctn-kix_6nqn63a3krj0-3 0}ol.lst-kix_list_1-3.start{counter-reset:lst-ctn-kix_list_1-3 0}ul.lst-kix_list_2-8{list-style-type:none}ol.lst-kix_list_1-2.start{counter-reset:lst-ctn-kix_list_1-2 0}ul.lst-kix_list_2-2{list-style-type:none}ul.lst-kix_list_2-3{list-style-type:none}ul.lst-kix_list_2-1{list-style-type:none}ul.lst-kix_list_2-6{list-style-type:none}.lst-kix_list_1-1>li:before{content:" "}ul.lst-kix_list_2-7{list-style-type:none}ol.lst-kix_1gm9o9wcd7pg-4.start{counter-reset:lst-ctn-kix_1gm9o9wcd7pg-4 0}ul.lst-kix_list_2-4{list-style-type:none}ul.lst-kix_list_2-5{list-style-type:none}.lst-kix_list_1-3>li:before{content:" "}.lst-kix_6wajunywya39-8>li{counter-increment:lst-ctn-kix_6wajunywya39-8}ol.lst-kix_1gm9o9wcd7pg-5.start{counter-reset:lst-ctn-kix_1gm9o9wcd7pg-5 0}.lst-kix_list_1-7>li:before{content:" "}ol.lst-kix_ex0q9wj3jmqv-3.start{counter-reset:lst-ctn-kix_ex0q9wj3jmqv-3 0}ol.lst-kix_a9i1uwuflx9z-0.start{counter-reset:lst-ctn-kix_a9i1uwuflx9z-0 0}.lst-kix_list_1-3>li{counter-increment:lst-ctn-kix_list_1-3}.lst-kix_list_1-5>li:before{content:" "}.lst-kix_6wajunywya39-2>li{counter-increment:lst-ctn-kix_6wajunywya39-2}.lst-kix_6wajunywya39-2>li:before{content:"" counter(lst-ctn-kix_6wajunywya39-2,lower-roman) ". "}ol.lst-kix_6nqn63a3krj0-4.start{counter-reset:lst-ctn-kix_6nqn63a3krj0-4 0}.lst-kix_list_2-1>li:before{content:" "}.lst-kix_f0cuw8wlviqi-3>li{counter-increment:lst-ctn-kix_f0cuw8wlviqi-3}.lst-kix_6wajunywya39-4>li:before{content:"" counter(lst-ctn-kix_6wajunywya39-4,lower-latin) ". "}.lst-kix_list_2-3>li:before{content:" "}.lst-kix_k49x0wlmrunk-4>li{counter-increment:lst-ctn-kix_k49x0wlmrunk-4}.lst-kix_6wajunywya39-8>li:before{content:"" counter(lst-ctn-kix_6wajunywya39-8,lower-roman) ". "}.lst-kix_6wajunywya39-6>li:before{content:"" counter(lst-ctn-kix_6wajunywya39-6,decimal) ". "}.lst-kix_6wajunywya39-7>li:before{content:"" counter(lst-ctn-kix_6wajunywya39-7,lower-latin) ". "}ol.lst-kix_6wajunywya39-2.start{counter-reset:lst-ctn-kix_6wajunywya39-2 0}ol.lst-kix_list_3-0{list-style-type:none}.lst-kix_1gm9o9wcd7pg-2>li{counter-increment:lst-ctn-kix_1gm9o9wcd7pg-2}.lst-kix_list_1-1>li{counter-increment:lst-ctn-kix_list_1-1}ol.lst-kix_a9i1uwuflx9z-5.start{counter-reset:lst-ctn-kix_a9i1uwuflx9z-5 0}.lst-kix_list_3-0>li:before{content:"" counter(lst-ctn-kix_list_3-0,decimal) ". "}.lst-kix_6nqn63a3krj0-6>li{counter-increment:lst-ctn-kix_6nqn63a3krj0-6}.lst-kix_list_3-1>li:before{content:" "}.lst-kix_list_3-2>li:before{content:" "}.lst-kix_6wajunywya39-0>li:before{content:"" counter(lst-ctn-kix_6wajunywya39-0,decimal) ". "}ol.lst-kix_list_1-8.start{counter-reset:lst-ctn-kix_list_1-8 0}.lst-kix_list_4-0>li{counter-increment:lst-ctn-kix_list_4-0}.lst-kix_list_3-5>li:before{content:" "}.lst-kix_ex0q9wj3jmqv-1>li{counter-increment:lst-ctn-kix_ex0q9wj3jmqv-1}ol.lst-kix_k49x0wlmrunk-3.start{counter-reset:lst-ctn-kix_k49x0wlmrunk-3 0}.lst-kix_list_3-4>li:before{content:" "}.lst-kix_f0cuw8wlviqi-8>li{counter-increment:lst-ctn-kix_f0cuw8wlviqi-8}.lst-kix_list_3-3>li:before{content:" "}.lst-kix_6wajunywya39-6>li{counter-increment:lst-ctn-kix_6wajunywya39-6}ol.lst-kix_6nqn63a3krj0-5.start{counter-reset:lst-ctn-kix_6nqn63a3krj0-5 0}ol.lst-kix_f0cuw8wlviqi-8.start{counter-reset:lst-ctn-kix_f0cuw8wlviqi-8 0}ol.lst-kix_ex0q9wj3jmqv-8.start{counter-reset:lst-ctn-kix_ex0q9wj3jmqv-8 0}.lst-kix_1gm9o9wcd7pg-0>li{counter-increment:lst-ctn-kix_1gm9o9wcd7pg-0}.lst-kix_list_3-8>li:before{content:" "}ol.lst-kix_6nqn63a3krj0-0.start{counter-reset:lst-ctn-kix_6nqn63a3krj0-0 0}.lst-kix_list_2-0>li{counter-increment:lst-ctn-kix_list_2-0}.lst-kix_list_3-6>li:before{content:" "}.lst-kix_list_3-7>li:before{content:" "}.lst-kix_k49x0wlmrunk-6>li{counter-increment:lst-ctn-kix_k49x0wlmrunk-6}.lst-kix_a9i1uwuflx9z-1>li{counter-increment:lst-ctn-kix_a9i1uwuflx9z-1}.lst-kix_6wajunywya39-4>li{counter-increment:lst-ctn-kix_6wajunywya39-4}.lst-kix_6nqn63a3krj0-8>li{counter-increment:lst-ctn-kix_6nqn63a3krj0-8}ol.lst-kix_6nqn63a3krj0-6.start{counter-reset:lst-ctn-kix_6nqn63a3krj0-6 0}ol.lst-kix_list_2-0{list-style-type:none}.lst-kix_list_4-8>li:before{content:" "}ol.lst-kix_k49x0wlmrunk-7{list-style-type:none}ol.lst-kix_k49x0wlmrunk-8{list-style-type:none}.lst-kix_list_4-7>li:before{content:" "}.lst-kix_6nqn63a3krj0-4>li{counter-increment:lst-ctn-kix_6nqn63a3krj0-4}.lst-kix_f0cuw8wlviqi-4>li{counter-increment:lst-ctn-kix_f0cuw8wlviqi-4}ul.lst-kix_list_4-8{list-style-type:none}.lst-kix_ex0q9wj3jmqv-8>li{counter-increment:lst-ctn-kix_ex0q9wj3jmqv-8}ol.lst-kix_k49x0wlmrunk-0{list-style-type:none}ul.lst-kix_list_4-6{list-style-type:none}ol.lst-kix_k49x0wlmrunk-1{list-style-type:none}ul.lst-kix_list_4-7{list-style-type:none}ol.lst-kix_k49x0wlmrunk-2{list-style-type:none}ol.lst-kix_k49x0wlmrunk-3{list-style-type:none}ol.lst-kix_k49x0wlmrunk-4{list-style-type:none}ol.lst-kix_k49x0wlmrunk-5{list-style-type:none}ol.lst-kix_k49x0wlmrunk-6{list-style-type:none}ul.lst-kix_list_4-1{list-style-type:none}ol.lst-kix_6wajunywya39-8.start{counter-reset:lst-ctn-kix_6wajunywya39-8 0}ul.lst-kix_list_4-4{list-style-type:none}ul.lst-kix_list_4-5{list-style-type:none}ol.lst-kix_k49x0wlmrunk-8.start{counter-reset:lst-ctn-kix_k49x0wlmrunk-8 0}ul.lst-kix_list_4-2{list-style-type:none}ul.lst-kix_list_4-3{list-style-type:none}.lst-kix_a9i1uwuflx9z-6>li{counter-increment:lst-ctn-kix_a9i1uwuflx9z-6}.lst-kix_a9i1uwuflx9z-3>li{counter-increment:lst-ctn-kix_a9i1uwuflx9z-3}ol.lst-kix_a9i1uwuflx9z-4.start{counter-reset:lst-ctn-kix_a9i1uwuflx9z-4 0}ol.lst-kix_6wajunywya39-1.start{counter-reset:lst-ctn-kix_6wajunywya39-1 0}.lst-kix_k49x0wlmrunk-0>li:before{content:"" counter(lst-ctn-kix_k49x0wlmrunk-0,decimal) ". "}.lst-kix_k49x0wlmrunk-1>li:before{content:"" counter(lst-ctn-kix_k49x0wlmrunk-1,lower-latin) ". "}.lst-kix_k49x0wlmrunk-8>li{counter-increment:lst-ctn-kix_k49x0wlmrunk-8}.lst-kix_1gm9o9wcd7pg-4>li{counter-increment:lst-ctn-kix_1gm9o9wcd7pg-4}ol.lst-kix_6wajunywya39-7.start{counter-reset:lst-ctn-kix_6wajunywya39-7 0}.lst-kix_k49x0wlmrunk-2>li{counter-increment:lst-ctn-kix_k49x0wlmrunk-2}ol.lst-kix_6nqn63a3krj0-1.start{counter-reset:lst-ctn-kix_6nqn63a3krj0-1 0}.lst-kix_k49x0wlmrunk-3>li:before{content:"" counter(lst-ctn-kix_k49x0wlmrunk-3,decimal) ". "}.lst-kix_k49x0wlmrunk-2>li:before{content:"" counter(lst-ctn-kix_k49x0wlmrunk-2,lower-roman) ". "}ul.lst-kix_v16i7znt6euc-6{list-style-type:none}.lst-kix_k49x0wlmrunk-5>li:before{content:"" counter(lst-ctn-kix_k49x0wlmrunk-5,lower-roman) ". "}.lst-kix_list_2-6>li:before{content:" "}.lst-kix_v16i7znt6euc-1>li:before{content:"-  "}ul.lst-kix_v16i7znt6euc-5{list-style-type:none}ul.lst-kix_v16i7znt6euc-8{list-style-type:none}ol.lst-kix_ex0q9wj3jmqv-8{list-style-type:none}ul.lst-kix_v16i7znt6euc-7{list-style-type:none}ol.lst-kix_ex0q9wj3jmqv-7{list-style-type:none}ol.lst-kix_ex0q9wj3jmqv-7.start{counter-reset:lst-ctn-kix_ex0q9wj3jmqv-7 0}.lst-kix_list_2-4>li:before{content:" "}.lst-kix_list_2-8>li:before{content:" "}ol.lst-kix_ex0q9wj3jmqv-6{list-style-type:none}ol.lst-kix_ex0q9wj3jmqv-5{list-style-type:none}.lst-kix_ex0q9wj3jmqv-8>li:before{content:"" counter(lst-ctn-kix_ex0q9wj3jmqv-8,lower-roman) ". "}.lst-kix_ex0q9wj3jmqv-4>li:before{content:"" counter(lst-ctn-kix_ex0q9wj3jmqv-4,lower-latin) ". "}ol.lst-kix_ex0q9wj3jmqv-4{list-style-type:none}ul.lst-kix_v16i7znt6euc-0{list-style-type:none}ul.lst-kix_v16i7znt6euc-2{list-style-type:none}ol.lst-kix_k49x0wlmrunk-2.start{counter-reset:lst-ctn-kix_k49x0wlmrunk-2 0}.lst-kix_k49x0wlmrunk-7>li:before{content:"" counter(lst-ctn-kix_k49x0wlmrunk-7,lower-latin) ". "}.lst-kix_f0cuw8wlviqi-1>li{counter-increment:lst-ctn-kix_f0cuw8wlviqi-1}ul.lst-kix_v16i7znt6euc-1{list-style-type:none}.lst-kix_v16i7znt6euc-3>li:before{content:"-  "}.lst-kix_ex0q9wj3jmqv-6>li:before{content:"" counter(lst-ctn-kix_ex0q9wj3jmqv-6,decimal) ". "}ul.lst-kix_v16i7znt6euc-4{list-style-type:none}.lst-kix_a9i1uwuflx9z-5>li{counter-increment:lst-ctn-kix_a9i1uwuflx9z-5}ul.lst-kix_v16i7znt6euc-3{list-style-type:none}ol.lst-kix_f0cuw8wlviqi-1{list-style-type:none}ol.lst-kix_f0cuw8wlviqi-0{list-style-type:none}.lst-kix_6nqn63a3krj0-1>li:before{content:"" counter(lst-ctn-kix_6nqn63a3krj0-1,lower-latin) ". "}ol.lst-kix_f0cuw8wlviqi-4.start{counter-reset:lst-ctn-kix_f0cuw8wlviqi-4 0}.lst-kix_6wajunywya39-0>li{counter-increment:lst-ctn-kix_6wajunywya39-0}.lst-kix_ex0q9wj3jmqv-0>li:before{content:"" counter(lst-ctn-kix_ex0q9wj3jmqv-0,decimal) ". "}.lst-kix_v16i7znt6euc-5>li:before{content:"-  "}ol.lst-kix_f0cuw8wlviqi-8{list-style-type:none}.lst-kix_6nqn63a3krj0-5>li:before{content:"" counter(lst-ctn-kix_6nqn63a3krj0-5,lower-roman) ". "}ol.lst-kix_f0cuw8wlviqi-7{list-style-type:none}ol.lst-kix_f0cuw8wlviqi-6{list-style-type:none}ol.lst-kix_f0cuw8wlviqi-5{list-style-type:none}.lst-kix_v16i7znt6euc-7>li:before{content:"-  "}ol.lst-kix_f0cuw8wlviqi-4{list-style-type:none}.lst-kix_6nqn63a3krj0-3>li:before{content:"" counter(lst-ctn-kix_6nqn63a3krj0-3,decimal) ". "}.lst-kix_list_1-7>li{counter-increment:lst-ctn-kix_list_1-7}ol.lst-kix_f0cuw8wlviqi-3{list-style-type:none}ol.lst-kix_f0cuw8wlviqi-2{list-style-type:none}.lst-kix_ex0q9wj3jmqv-2>li:before{content:"" counter(lst-ctn-kix_ex0q9wj3jmqv-2,lower-roman) ". "}.lst-kix_a9i1uwuflx9z-6>li:before{content:"" counter(lst-ctn-kix_a9i1uwuflx9z-6,decimal) ". "}.lst-kix_a9i1uwuflx9z-8>li:before{content:"" counter(lst-ctn-kix_a9i1uwuflx9z-8,lower-roman) ". "}ol.lst-kix_k49x0wlmrunk-5.start{counter-reset:lst-ctn-kix_k49x0wlmrunk-5 0}.lst-kix_list_4-0>li:before{content:"" counter(lst-ctn-kix_list_4-0,decimal) ". "}ol.lst-kix_f0cuw8wlviqi-1.start{counter-reset:lst-ctn-kix_f0cuw8wlviqi-1 0}ol.lst-kix_list_1-7.start{counter-reset:lst-ctn-kix_list_1-7 0}.lst-kix_list_4-4>li:before{content:" "}.lst-kix_a9i1uwuflx9z-0>li:before{content:"" counter(lst-ctn-kix_a9i1uwuflx9z-0,decimal) ") "}.lst-kix_list_1-5>li{counter-increment:lst-ctn-kix_list_1-5}.lst-kix_1gm9o9wcd7pg-8>li{counter-increment:lst-ctn-kix_1gm9o9wcd7pg-8}.lst-kix_a9i1uwuflx9z-4>li{counter-increment:lst-ctn-kix_a9i1uwuflx9z-4}.lst-kix_list_4-2>li:before{content:" "}.lst-kix_list_4-6>li:before{content:" "}.lst-kix_6nqn63a3krj0-2>li{counter-increment:lst-ctn-kix_6nqn63a3krj0-2}ol.lst-kix_ex0q9wj3jmqv-3{list-style-type:none}ol.lst-kix_ex0q9wj3jmqv-2{list-style-type:none}.lst-kix_a9i1uwuflx9z-2>li:before{content:"" counter(lst-ctn-kix_a9i1uwuflx9z-2,lower-roman) ") "}.lst-kix_a9i1uwuflx9z-4>li:before{content:"(" counter(lst-ctn-kix_a9i1uwuflx9z-4,lower-latin) ") "}ol.lst-kix_ex0q9wj3jmqv-1{list-style-type:none}ol.lst-kix_1gm9o9wcd7pg-0.start{counter-reset:lst-ctn-kix_1gm9o9wcd7pg-0 0}ol.lst-kix_ex0q9wj3jmqv-0{list-style-type:none}ol.lst-kix_list_4-0{list-style-type:none}ol.lst-kix_f0cuw8wlviqi-2.start{counter-reset:lst-ctn-kix_f0cuw8wlviqi-2 0}.lst-kix_ex0q9wj3jmqv-3>li{counter-increment:lst-ctn-kix_ex0q9wj3jmqv-3}.lst-kix_k49x0wlmrunk-7>li{counter-increment:lst-ctn-kix_k49x0wlmrunk-7}ol.lst-kix_6nqn63a3krj0-0{list-style-type:none}ol.lst-kix_6nqn63a3krj0-1{list-style-type:none}ol.lst-kix_6nqn63a3krj0-2{list-style-type:none}ol.lst-kix_6nqn63a3krj0-3{list-style-type:none}ol.lst-kix_6nqn63a3krj0-4{list-style-type:none}ol.lst-kix_6nqn63a3krj0-5{list-style-type:none}.lst-kix_k49x0wlmrunk-1>li{counter-increment:lst-ctn-kix_k49x0wlmrunk-1}ol.lst-kix_6nqn63a3krj0-6{list-style-type:none}ol.lst-kix_6nqn63a3krj0-7{list-style-type:none}ol.lst-kix_6nqn63a3krj0-8{list-style-type:none}ol.lst-kix_a9i1uwuflx9z-6.start{counter-reset:lst-ctn-kix_a9i1uwuflx9z-6 0}.lst-kix_6nqn63a3krj0-3>li{counter-increment:lst-ctn-kix_6nqn63a3krj0-3}ol.lst-kix_1gm9o9wcd7pg-0{list-style-type:none}.lst-kix_list_1-0>li:before{content:" "}.lst-kix_k49x0wlmrunk-0>li{counter-increment:lst-ctn-kix_k49x0wlmrunk-0}ol.lst-kix_1gm9o9wcd7pg-3{list-style-type:none}ol.lst-kix_1gm9o9wcd7pg-4{list-style-type:none}.lst-kix_list_1-2>li:before{content:" "}ol.lst-kix_list_2-0.start{counter-reset:lst-ctn-kix_list_2-0 0}ol.lst-kix_1gm9o9wcd7pg-1{list-style-type:none}ol.lst-kix_a9i1uwuflx9z-7{list-style-type:none}ol.lst-kix_1gm9o9wcd7pg-2{list-style-type:none}ol.lst-kix_a9i1uwuflx9z-8{list-style-type:none}ol.lst-kix_1gm9o9wcd7pg-7{list-style-type:none}ol.lst-kix_a9i1uwuflx9z-5{list-style-type:none}ol.lst-kix_1gm9o9wcd7pg-8{list-style-type:none}ol.lst-kix_a9i1uwuflx9z-6{list-style-type:none}.lst-kix_list_1-4>li:before{content:" "}ol.lst-kix_1gm9o9wcd7pg-5{list-style-type:none}ol.lst-kix_a9i1uwuflx9z-3{list-style-type:none}ol.lst-kix_1gm9o9wcd7pg-6{list-style-type:none}.lst-kix_6wajunywya39-5>li{counter-increment:lst-ctn-kix_6wajunywya39-5}ol.lst-kix_a9i1uwuflx9z-4{list-style-type:none}ol.lst-kix_a9i1uwuflx9z-1{list-style-type:none}.lst-kix_list_1-0>li{counter-increment:lst-ctn-kix_list_1-0}ol.lst-kix_a9i1uwuflx9z-2{list-style-type:none}ol.lst-kix_a9i1uwuflx9z-0{list-style-type:none}.lst-kix_f0cuw8wlviqi-6>li{counter-increment:lst-ctn-kix_f0cuw8wlviqi-6}.lst-kix_list_1-6>li{counter-increment:lst-ctn-kix_list_1-6}.lst-kix_list_1-6>li:before{content:" "}.lst-kix_6wajunywya39-1>li:before{content:"" counter(lst-ctn-kix_6wajunywya39-1,lower-latin) ". "}.lst-kix_6wajunywya39-3>li:before{content:"" counter(lst-ctn-kix_6wajunywya39-3,decimal) ". "}.lst-kix_list_2-0>li:before{content:"" counter(lst-ctn-kix_list_2-0,decimal) ". "}ol.lst-kix_f0cuw8wlviqi-3.start{counter-reset:lst-ctn-kix_f0cuw8wlviqi-3 0}.lst-kix_6wajunywya39-5>li:before{content:"" counter(lst-ctn-kix_6wajunywya39-5,lower-roman) ". "}.lst-kix_list_1-8>li:before{content:" "}.lst-kix_list_2-2>li:before{content:" "}.lst-kix_f0cuw8wlviqi-0>li{counter-increment:lst-ctn-kix_f0cuw8wlviqi-0}ol.lst-kix_k49x0wlmrunk-4.start{counter-reset:lst-ctn-kix_k49x0wlmrunk-4 0}ol.lst-kix_a9i1uwuflx9z-7.start{counter-reset:lst-ctn-kix_a9i1uwuflx9z-7 0}ol{margin:0;padding:0}table td,table th{padding:0}.c0{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:122.8pt;border-top-color:#000000;border-bottom-style:solid}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:justify;height:12pt}.c11{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:justify}.c18{color:#ff0000;font-weight:400;text-decoration:none;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c14{color:#000000;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Times New Roman";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Times New Roman";font-style:normal}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c16{color:#000000;text-decoration:none;vertical-align:baseline;font-family:"Times New Roman";font-style:normal}.c27{color:#000000;font-weight:400;text-decoration:none;font-family:"Times New Roman";font-style:italic}.c22{border-spacing:0;border-collapse:collapse;margin-right:auto}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c29{background-color:#ffffff;max-width:491pt;padding:72pt 60.5pt 72pt 60.5pt}.c2{font-size:10pt;color:#1155cc;text-decoration:underline}.c20{margin-left:36pt;padding-left:0pt}.c5{margin-left:43pt;margin-right:47.5pt}.c19{color:inherit;text-decoration:inherit}.c8{orphans:2;widows:2}.c23{padding:0;margin:0}.c21{font-size:14pt}.c9{font-size:10pt}.c28{font-size:11pt}.c17{vertical-align:baseline}.c24{font-size:6pt}.c25{text-indent:13.5pt}.c15{height:12pt}.c13{height:0pt}.c12{font-weight:700}.c26{font-size:18pt}.title{padding-top:0pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:0pt;font-family:"Federo";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:center}.subtitle{padding-top:12pt;color:#666666;font-size:14pt;padding-bottom:6pt;font-family:"Arial";line-height:1.0;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:center}li{color:#000000;font-size:12pt;font-family:"Times New Roman"}p{margin:0;color:#000000;font-size:12pt;font-family:"Times New Roman"}h1{padding-top:3pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:0pt;font-family:"Federo";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:2pt;color:#000000;font-size:11pt;padding-bottom:0pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c29"><div><p class="c4 c8 c15"><span class="c17 c18"></span></p></div><hr><p class="c3 c15"><span class="c16 c12 c21"></span></p><p class="c3 c15"><span class="c16 c9 c12"></span></p><p class="c3"><span class="c12 c26">FAKE NEWS CHALLENGE</span><hr></p><p class="c3 c15"><span class="c10 c9"></span></p><p class="c3"><span class="c16 c9 c12">Richard Godden</span></p><p class="c3"><span class="c10 c9">rg3047@columbia.edu</span></p><p class="c3"><span class="c10 c9">github.com/goddenrich</span></p><p class="c3 c15"><span class="c10 c9"></span></p><p class="c3"><span class="c16 c9 c12">Oriana Fuentes </span></p><p class="c3"><span class="c10 c9">oif2102@columbia.edu</span></p><p class="c3"><span class="c10 c9">github.com/fuentesori</span></p><p class="c3 c15"><span class="c10 c9"></span></p><p class="c3 c15"><span class="c16 c9 c12"></span></p><p class="c3"><span class="c16 c9 c12">Jonathan Chan</span></p><p class="c3"><span class="c10 c9">jc4659@columbia.edu</span></p><p class="c3"><span class="c10 c9">github.com/jchanaf</span></p><p class="c3 c15"><span class="c10 c9"></span></p><p class="c3"><span class="c16 c9 c12">Killian Robert Rutherford</span></p><p class="c3"><span class="c10 c9">krr2125@columbia.edu</span></p><p class="c3"><span class="c10 c9">github.com/killianrutherford</span></p><p class="c3 c15"><span class="c10 c9"></span></p><p class="c4 c8 c15"><span class="c7"></span></p><p class="c3 c5"><span class="c17 c12">Abstract</span></p><p class="c6 c5"><span class="c9 c17 c27">Fake news has recently risen to the forefront of society due to its perceived influence on recent political events. The problem of discerning fact from fiction is by no means new, however we find ourselves at a point in history where we have enough data from connected sources on the internet to start to tackle the challenge using machine learning. The Fake news challenge[1] was established to explore how machine learning and AI techniques could be employed to automate the current manual, fact checking process. The organizers of the challenge have provided a baseline which uses hand crafted features designed specifically for the task. We would like to explore techniques using pretrained word2vec[2][3] embeddings and not use hand crafted features. We have shown many methods can be applied to this task including ensemble methods and recurrent neural networks. We found that it was possible to beat the baseline focusing on SVD and Bag of Words models to fit into SVM classifiers.</span></p><p class="c4 c8 c15"><span class="c11"></span></p><p class="c4 c8"><span class="c17 c12">1. Introduction</span></p><p class="c1 c25"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">The Fake News Challenge was established to explore how machine learning and AI techniques could be employed to automate the, current manual, fact checking process. This process is a complex pipeline which can be broken down into subtasks. The first step in this process is to identify articles that support or refute a claim. To do this we have to determine stance, validity and association between texts. The challenge is to determine the stance of two pieces of text to a topic, claim or issue. The organizers chose the task of determining the stance between an excerpt from a news article and a headline to determine whether the headline and body agree, disagree, discuss or are unrelated to each other. </span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 325.83px; height: 208.84px;"><img alt="" src="images/image6.png" style="width: 325.83px; height: 208.84px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 186.31px; height: 218.50px;"><img alt="" src="images/image7.png" style="width: 186.31px; height: 218.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c9 c12">Fig 1.1</span><span class="c9">&nbsp;The classification task &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c9 c12">Fig 1.2</span><span class="c10 c9">&nbsp;The scoring mechanism</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">The 4 classes have been defined as follows:</span></p><ol class="c23 lst-kix_a9i1uwuflx9z-0 start" start="1"><li class="c6 c20"><span class="c10 c9">Agrees: The body text agrees with the headline</span></li><li class="c6 c20"><span class="c10 c9">Disagrees: The body text disagrees with the headline</span></li><li class="c6 c20"><span class="c10 c9">Discusses: The body text discusses the same topic as the headline, but does not take a position</span></li><li class="c6 c20"><span class="c10 c9">Unrelated: The body text discusses a different topic than the headline</span></li></ol><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">The task of deciding whether a body &nbsp;is unrelated or related to a headline is a much easier task than deciding on the three other classifications. To reflect this, the scoring mechanism gives more weight towards correctly identifying whether an article agrees, disagrees or discusses rather than identifying unrelated header and bodies [1].</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">The organisers of the challenge provided a baseline using hand-coded features designed for this task and used a Gradient boosting classifier which obtains a test score (based on the scoring described above) of 79.53%. One issue with the dataset which we will discuss below is that a naive splitting of train and test such that the baseline provides means that articles that appear in the training set will also appear in the test set. We call this bleeding. Without allowing bleeding the baseline obtains a test score of 76.69%.</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 290.83px; height: 208.94px;"><img alt="Screen Shot 2017-04-30 at 22.28.15.png" src="images/image5.png" style="width: 290.83px; height: 208.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c9 c12">Fig 1.3</span><span class="c9">&nbsp;confusion matrix of the baseline model on the bleeded data set. The rows indicate the ground truth labels, and the columns are the model predicted labels. The baseline does a reasonable job of discerning related and unrelated but is not very good at classifying correctly within the related classes.</span></p><p class="c1 c25"><span class="c10 c9"></span></p><p class="c6"><span class="c11">2. Data</span></p><p class="c1"><span class="c14 c12"></span></p><p class="c6"><span class="c11">2.1. Fake News Challenge Dataset</span></p><p class="c1"><span class="c14 c12"></span></p><p class="c6"><span class="c10 c9">The data provided by Fake News Challenge consists of 1,647 unique headlines and 1,682 unique articles bodies. These are associated with each other and then tagged as: agree, disagree, discuss or unrelated. Below is the distribution of the 49,972 associations between the different headlines and bodies:</span></p><p class="c6"><span class="c24"><br></span><span class="c10 c9">agree: 3,678<br>disagree: 840<br>discuss: 8,909<br>unrelated: 36,545</span></p><p class="c6"><span class="c24"><br></span><span class="c10 c9">As the data stands, it cannot be easily split into test and training for cross-validation purposes due to the fact that headlines are associated across several bodies and vice-versa. When splitting the data naively, bodies or headlines will &lsquo;bleed&rsquo; into the test sets as they will have most likely been trained upon already. Additionally, when considering the unique bodies and headlines, rather than the set of associations, we encounter a relatively small sample size. <br>In order to remedy the &lsquo;bleeding&rsquo; of headlines and bodies across training and test sets, we will pick a subset of headlines to train and remove the headlines that are associated with these from the set that will be tested upon. In order to remedy the limited amount of unique bodies and headlines, we will augment the dataset by generating new bodies and headlines with synonyms and related words.<br></span></p><p class="c6"><span class="c11">2.2 Data Augmentation</span></p><p class="c1"><span class="c14 c12"></span></p><p class="c6"><span class="c16 c9 c12">2.2.1 Windowing</span></p><p class="c6"><span class="c10 c9">In order to produce more data points to train on we split the article bodies into a list of strings. Each window was set to be 150 tokens long and then the window is slided 75 words over. The overlapping window ensures that each window is of a reasonable size so that we didn&rsquo;t have article bodies with few or singleton words.</span></p><p class="c1"><span class="c14 c12"></span></p><p class="c6"><span class="c16 c9 c12">2.2.2 Word substitution</span></p><p class="c6"><span class="c10 c9">Another method that we used to increase the size of the dataset was word substitution of the headlines based on a technique proposed by Vijayaraghavan [4]. We could have run this on the bodies as well, but we were able to generate a dataset magnitudes larger than the original with the windowing and word substitution on headlines strategy. For a given headline, we select target words that are not common stoplist words and are not compound words, which we approximated by checking whether consecutive pairs of words in the headline were in the word2vec vocabulary. For each target word, we generated a candidate list of synonyms, hyponyms, hypernyms in the Natural Language Toolkit (NLTK) corpus and then compared the target word2vec representation and the candidate word using cosine similarity, narrowing down the candidates to the 10 with the highest similarity above a certain threshold. We found that .34 was a reasonable threshold. Once we obtained this list of candidate words, we randomly selected target words in each headline and randomly picked words in the candidate list to replace it with. We did this multiple times for each headline in the dataset in order to generated a significantly larger dataset. </span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c11">3. Methods</span></p><p class="c1"><span class="c14 c12"></span></p><p class="c6"><span class="c11">3.1 data processing</span></p><p class="c1"><span class="c14 c12"></span></p><p class="c6"><span class="c9">The original data is processed by </span><span class="c9 c12">data_splitting.py</span><span class="c10 c9">&nbsp;which has a series of tools and methods to handle the different steps of the process shown below:</span></p><p class="c4 c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 641.00px; height: 408.50px;"><img alt="dataProcessing (1).png" src="images/image8.png" style="width: 641.00px; height: 408.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c9 c12">Figure 3.1.1</span><span class="c10 c9">: Flow chart depicts the data processing pipeline</span></p><p class="c4 c8 c15"><span class="c10 c9"></span></p><p class="c4 c8"><span class="c9">The original .csv files from the challenge,</span><span class="c9 c12">&nbsp;train_bodies</span><span class="c9">&nbsp;and </span><span class="c9 c12">train_stances</span><span class="c9">&nbsp;are passed to data_splitting which runs </span><span class="c9 c12">preprocess_all.</span><span class="c9">&nbsp;The first step is to run</span><span class="c9 c12">&nbsp;import_data</span><span class="c9">&nbsp;and convert the .csv files to pandas dataframes, run </span><span class="c9 c12">oneoff_cleanup</span><span class="c9">&nbsp;which removes only a handful of bodies which are not in english. Then the bodies are passed to </span><span class="c9 c12">window_splitting</span><span class="c9">&nbsp;which splits the bodies into a list of strings with a sliding window of 150 tokens with 75 token overlap. One converted into a list of strings, the bodies and stances are crossed with </span><span class="c9 c12">data_crossing</span><span class="c9">. The function </span><span class="c9 c12">data_crossing</span><span class="c9">&nbsp;will attach the bodies related to the body ids in the stances dataset, and re-indexes the bodies so they can be split by index to create test, training and cross validation sets. At this point a dataframe called </span><span class="c9 c12">df_all</span><span class="c9">&nbsp;is returned which contains the full panel of stances, headlines, and article bodies, along with additional indexing for headlines and bodies in order to split them efficiently and avoid bleeding. The dataframe </span><span class="c9 c12">df_all </span><span class="c9">is passed to </span><span class="c9 c12">even_split</span><span class="c9">&nbsp;which takes a subset of the body IDs and a subset of headline IDs, for example 70% and picks only the associations between those two 70% sets within the list of 49,972 associations. </span><span class="c9 c12">even_split </span><span class="c9">calls </span><span class="c9 c12">data_splitting </span><span class="c9">and </span><span class="c9 c12">check_splitting</span><span class="c10 c9">, which perform the splitting described above and then checks whether the distribution of stances (agree, disagree, etc) is close to the distribution of these in the original data. The dataframes are then saved into .csv files.</span></p><p class="c4 c8 c15"><span class="c10 c9"></span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c11">3.2 Ensemble methods </span></p><p class="c1"><span class="c12 c14"></span></p><p class="c6"><span class="c10 c9">The initial problem was, given the 300 state space word2vec (w2v) representation mentioned above, to reconstruct a representation of the header and body paragraphs. Several ideas were attempted, and fed into a variety of ensemble methods, with results detailed in the next section. All tests were conducted on non-bleeded datasets to attempt to obtain as accurate representations of the data as possible.</span></p><p class="c1"><span class="c10 c28"></span></p><p class="c6"><span class="c9 c12">3.2.1 SVD</span></p><p class="c6"><span class="c10 c9">The initial preprocessing step involved creating a matrix of vectors to represent the body and header paragraphs. For each word found in the w2v model, a vector was concatenated to the header/body matrix, to give a final matrix representation of each paragraph with dimensions (300, #valid words). </span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c9">Standard kSVD was applied to each header and body matrix to obtain the top k eigenvector representatives of the header/body. Primarily dealing with the top eigenvector, as well as the average of top k eigenvectors, cosine similarity was compared between header and body to attempt to identify linear ranges between the four classes: unrelated, discuss, agree, disagree. Although this gave some sort of indication as to whether the header and body were related, it did not yield conclusive results on a four-way classification, with predictions between agree, disagree, and discuss rarely performing better than random.</span></p><p class="c1"><span class="c16 c9 c12"></span></p><p class="c6"><span class="c9 c12">3.2.2 SVM and Ensemble</span></p><p class="c6"><span class="c10 c9">Given the top eigenvector (ranked by decreasing value of eigenvalue) for header and body, this could be fed into a specific method as a feature vector.</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">Concatenation and subtraction of header-body eigenvectors were attempted, (600 and 300 feature vector respectively); a hyperparameter grid search was conducted over a variety of different methods and method parameters to obtain the results in section 4.1.1.</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c9 c12">3.2.3 Bag of Words</span></p><p class="c6"><span class="c10 c9">Following a method proposed by [5], stop words are primarily not included in any of the above matrix representations for header and body. The body is split up into its respective sentences, and each sentence is represented by an average word vec of words in that sentence. The average head word vector is then calculated for these matrices, and cosine similarities of all the body sentence word vectors and the average head word vec are compared. The three body sentence word-vectors with the highest cosine similarity are concatenated, along with the average head vector and fed into a classifier (dimension of 1200).</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">This bag of words method in the original paper boasted high score metrics (in the 80 percents). Taking some of the top performing ensemble methods from the previous section, different hyperparameters were experimented with, as well as the number of sentence vectors to be concatenated. As can be seen in section 4.1.2 some of our best scores were able to be retrieved.</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c16 c9 c12">3.2.4 Penalty and Augmentation</span></p><p class="c6"><span class="c10 c9">In an attempt to incorporate ideas from previous sections, the final outlook on the problem involved mixing SVD eigenvectors and average word vectors together. The top eigenvector and average vector for both header and body were concatenated together to give a feature vector of size (1200). The training data was primarily augmented by splitting each body paragraph into two and assigning the same class with headers. </span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">In addition to this, we incorporated various ideas of feature selection involving L1 penalties learnt in linear LASSO methods to these non-linear situations. However, feature selection given non-linear kernel SVM&rsquo;s has always proven to be hard. Although some methods have been performed, using attempts such as hybrid recursive feature extraction algorithms, we leave such complicated recursive implementations for further improvement work [6]. </span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">Instead, we first feature-select using tree based methods, and trim the input data to include only relevant features, before passing it into the the best performing classifiers above, namely polynomial and RBF kernel SVM as well as KNN (see results in 4.1.3).</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">The final trials on the data involved solely focusing on svm polynomial kernel (which has consistently been the most performant model). A larger hyperparameter grid search was performed to select the best model parameters: &nbsp; </span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 415.00px; height: 53.00px;"><img alt="" src="images/image10.png" style="width: 415.00px; height: 53.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">Following this, a more sophisticated data augmentation method of windowing as described in previous data augmentation sections was utilised and applied both to the above concat/average mixed method, and the stanford bag of words model as well as incorporating feature selection, although this did not improve the performances that were already achieved up until this stage.</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c12">3</span><span class="c17 c12">.</span><span class="c12">3</span><span class="c17 c12">&nbsp;</span><span class="c11">LSTM models</span></p><p class="c1"><span class="c14 c12"></span></p><p class="c6"><span class="c10 c9">An approach with which to compare the above are neural networks. This would not require feature extraction. However, networks typically require a large amount of data to produce accurate results; to this end, we will augment the data as described above. This task is a sequence to label task and one appropriate model to handle these types of tasks is using recurrent neural networks and in particular long short term memory networks or LSTMs are adept to reasoning about over long dependent series. We decided to implement two versions of these networks that have been successful. One a bidirectional LSTM and in the other case, we add an attention layer which allows us to learn a weighting between the LSTM states at different points in the headline and in the body.</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c16 c9 c12">3.3.1 Bidirectional LSTM</span></p><p class="c6"><span class="c10 c9">A standard LSTM classifier was implemented using 100 hidden units and a 512 unit softmax classification layer as the baseline deep learning model. The LSTM cell is calculated with the usual input, forget, cell, and output states. The first is a bidirectional LSTM where the input data is read forwards by one layer of LSTM units and in reverse order by another layer of LSTM units. The outputs of each direction are concatenated before being used.</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 263.83px; height: 243.80px;"><img alt="bi_lstm1.jpg" src="images/image3.jpg" style="width: 263.83px; height: 243.80px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c9 c12">Fig 3.3.1.1</span><span class="c10 c9">&nbsp;visualization of the bidirectional lstm model [7]</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c9 c12">3.3.2 Bidirectional LSTM with attention</span></p><p class="c6"><span class="c10 c9">Previous work has been done in a similar problem, determining entailment, involving conditioning Long Short-Term Memory networks by Rocktaschel [8]. The entailment problem is understanding whether 2 sentences: entail each other, disagree with each other, or are unrelated. Their approach encodes each sentence using Word2Vec and passes each of them to separate LSTMs, with a slight modification. One of the LSTMs initializes the hidden state of the other LSTM with the last output of its hidden state. With this conditional encoding technique, they achieved 80% accuracy on the test set of Stanford Natural Language Inference corpus. </span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 440.74px; height: 198.50px;"><img alt="Screen Shot 2017-04-30 at 22.13.37.png" src="images/image2.png" style="width: 440.74px; height: 198.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c9 c12">Fig 3.3.2.1</span><span class="c9">&nbsp;visualization of the LSTM model with attention [8] </span></p><p class="c4 c8 c15"><span class="c7"></span></p><p class="c6"><span class="c10 c9">One challenge with the Fake News Challenge that neither the LSTM nor the bidirectional conditioned global attention LSTM approach handles is that the problem compares a sentence with a body of sentences. To address this, we had two approaches. In one, we simply limited the number of word2vec tokens in the body to 150. The other approach was to split the body of sentences into 150 word2vec tokens by running a sliding window across the body, feeding these with their respective headline to the neural network. </span></p><p class="c4 c8 c15"><span class="c7"></span></p><p class="c4 c8 c15"><span class="c7"></span></p><p class="c4 c8 c15"><span class="c7"></span></p><p class="c4 c8 c15"><span class="c7"></span></p><p class="c4 c8 c15"><span class="c7"></span></p><p class="c4 c8 c15"><span class="c7"></span></p><p class="c6"><span class="c12">4</span><span class="c17 c12">. </span><span class="c12">Results</span><span class="c17 c12">&nbsp;</span></p><p class="c1"><span class="c7"></span></p><p class="c6"><span class="c12">4</span><span class="c17 c12">.1. </span><span class="c11">Ensemble</span></p><p class="c1"><span class="c11"></span></p><p class="c6"><span class="c9 c12">4.1.1 SVM and Ensemble Methods</span></p><p class="c6"><span class="c10 c9">Preliminary results from hyperparameter and ensemble grid search using only top eigenvector from header and body gave the following: (The score percentage is measured according to the Fake News Challenge scoring [1]). (60/40 train/test and cross-validation used).</span></p><p class="c1"><span class="c16 c28 c12"></span></p><p class="c3"><span class="c10 c9">Table 1: Initial Hyperparameter Search Ensemble Methods for Subtraction and Concatenation of Header/Body SVD Eigenvectors</span></p><p class="c3 c15"><span class="c10 c9"></span></p><a id="t.9545415abaa6ac2a2672d0f1403b70c744b883d9"></a><a id="t.0"></a><table class="c22"><tbody><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">Ensemble method</span></p><p class="c4"><span class="c16 c9 c12">Splicing method</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">Training Accuracy</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">Testing Accuracy</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">Score percentage</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">Baseline</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">-</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">-</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">76.69</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">Random Forest:</span></p><p class="c4"><span class="c10 c9">Subtract / Concat</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">99.58/</span></p><p class="c4"><span class="c10 c9">97.97</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">73.00/</span></p><p class="c4"><span class="c10 c9">62.35</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">47.01/</span></p><p class="c4"><span class="c10 c9">44.93</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">Linear SVM:</span></p><p class="c4"><span class="c10 c9">Subtract/Concat</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">73.42/</span></p><p class="c4"><span class="c10 c9">73.42</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">71.60/</span></p><p class="c4"><span class="c10 c9">71.60</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">38.66/</span></p><p class="c4"><span class="c10 c9">38.66</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">Polynomial SVM:</span></p><p class="c4"><span class="c10 c9">Subtract/Concat</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">87.92/</span></p><p class="c4"><span class="c10 c9">95.45</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">76.16/</span></p><p class="c4"><span class="c16 c9 c12">81.78</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">52.38/</span></p><p class="c4"><span class="c16 c9 c12">65.79</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">RBF SVM:</span></p><p class="c4"><span class="c10 c9">Subtract/Concat</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">91.98/</span></p><p class="c4"><span class="c10 c9">91.86</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c9 c12">79.48</span><span class="c10 c9">/</span></p><p class="c4"><span class="c10 c9">77.28</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">60.23/</span></p><p class="c4"><span class="c10 c9">54.73</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">Decision Tree:</span></p><p class="c4"><span class="c10 c9">Subtract/Concat</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">100.00/</span></p><p class="c4"><span class="c10 c9">98.51</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">63.39/</span></p><p class="c4"><span class="c10 c9">45.19</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">48.63/</span></p><p class="c4"><span class="c10 c9">40.92</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">Subtract: K nearest neighbours</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">84.50/</span></p><p class="c4"><span class="c9 c10">84.79</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">67.94/</span></p><p class="c4"><span class="c10 c9">74.56</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">60.60/</span></p><p class="c4"><span class="c10 c9">62.08</span></p></td></tr></tbody></table><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">Interesting results can be seen, with greater testing accuracies and scores achieved with SVM&rsquo;s with RBF and Polynomial kernels, and K Nearest Neighbours. Although RBF SVM&rsquo;s seem to perform better for subtraction of eigenvectors, the final best result is with polynomial kernel SVM&rsquo;s with concatenation. This result of 65.8% is not nearly as good as the baseline model, which on the same non-bleeded datasets gives a performance of around 76.7%. However, given the simplicity of taking top SVD eigenvectors compared to the complex hand crafted features of the baseline models, these results give a promising first step.</span></p><p class="c1"><span class="c16 c28 c12"></span></p><p class="c6"><span class="c9 c12">4.1.2. Bag of Words</span></p><p class="c6"><span class="c10 c9">Taking the best performing models from the previous section, we attempt to fit the Bag of Words &nbsp;(BOW) model, monitoring and varying the number of top eigenvectors taken to feed into the classifier. The best results for each classifier are displayed below (with a 60/40 train/test split and cross-validation used).</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c1"><span class="c10 c9"></span></p><p class="c1"><span class="c10 c9"></span></p><p class="c1"><span class="c10 c9"></span></p><p class="c1"><span class="c10 c9"></span></p><p class="c1"><span class="c10 c9"></span></p><p class="c3"><span class="c10 c9">Table 2: Bag of Words Model for Ensemble Methods</span></p><p class="c1"><span class="c10 c9"></span></p><a id="t.a18842216622ceb10a887e54674f94aadeeda816"></a><a id="t.1"></a><table class="c22"><tbody><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">Ensemble method</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">Training Accuracy</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">Testing Accuracy</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c9 c12 c16">Score percentage</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">KNN</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">98.77</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">82.51</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">64.26</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">SVM Polynomial</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">99.97</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">86.41</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c9 c12">79.20</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">SVM RBF</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">98.99</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">85.94</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">74.90</span></p></td></tr></tbody></table><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">We see some very good results from this result, in fact improving over the baseline model by about 3 percent, without needing any hand crafted features whatsoever. This demonstrates a certain impressive conclusion regarding the properties of the word2vec model, in terms of how representative of a text the top 3 or even top 1 similar average sentence vectors are to the header.</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 299.46px; height: 215.50px;"><img alt="Screen Shot 2017-04-30 at 21.00.06.png" src="images/image4.png" style="width: 299.46px; height: 215.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c9 c12">Fig 4.1.2</span><span class="c10 c9">&nbsp;Bag of Words 86.41% accuracy and a total score of 79.20%. The rows indicate the ground truth labels, and the columns are the model predicted labels. </span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">As can be seen from the heat map, most classes are heavily classified correctly, although the most issues occur in the agree and disagree classes, most likely due to the lower number of training samples.</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c9 c12">4.1.3 Concat augment split half, penalisation feature selection</span></p><p class="c6"><span class="c10 c9">Mixing previous models, by concatenating the average word to vec and top eigenvector for header and bodies, as well as adding feature-selection and augmenting the data by splitting the bodies in half, we can retrieve very similar results to the BOW Model.</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c3"><span class="c10 c9">Table 3: Mixture of Average and SVD Eigenvector Concatenation with Feature Selection</span></p><a id="t.fb2e738946c8622c99822830246d3b71ab887b8c"></a><a id="t.2"></a><table class="c22"><tbody><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">Ensemble method</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">Training Accuracy</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">Testing Accuracy</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">Score percentage</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">SVM RBF</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">98.55</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">85.46</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">73.28</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">SVM Polynomial</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">98.60</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">86.09</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c16 c9 c12">79.19</span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">Random Forest</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">98.62</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">78.34</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">60.65</span></p><p class="c4 c15"><span class="c10 c9"></span></p></td></tr><tr class="c13"><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">KNN</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">88.82</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">73.99</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c4"><span class="c10 c9">66.30</span></p></td></tr></tbody></table><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">Interestingly, the KNN model performs better than in the BOW model. However, the best result, which comes again from a SVM polynomial kernel is just a very slightly lower score than the BOW model, but performs much better than the baseline.</span></p><p class="c1"><span class="c7"></span></p><p class="c6"><span class="c12">4</span><span class="c17 c12">.2. </span><span class="c11">LSTM</span></p><p class="c1"><span class="c16 c9 c12"></span></p><p class="c6"><span class="c10 c9">This section describes the results of the LSTM-based methods discussed in section 3.3. We present the confusion matrices for the baseline LSTM and the bidirectional conditioned global attention LSTM in Fig 4.2.1 and Fig 4.2.2 respectively. The baseline LSTM method had a higher raw accuracy than the bidirectional conditioned global attention LSTM, where accuracy is the unweighted classification accuracy. In contrast, the bidirectional conditioned global attention LSTM had a higher score, as calculated in the Fake News Challenge, which is a weighted classification score. </span></p><p class="c6"><span class="c10 c9">&nbsp;</span></p><p class="c6"><span class="c10 c9">Looking at the confusion matrix, this difference between accuracy and Fake News Challenge score is more apparent. The baseline LSTM has a tendency towards the majority class Unrelated. Disagree, which is the least represented in the dataset, is rarely predicted. Only 4 were predicted to be disagree out of 7536. While the bidirectional conditioned global attention LSTM had a lower accuracy, the model predicted a higher proportion of classes that were not unrelated, resulting in a higher score. </span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 393.00px; height: 261.50px;"><img alt="Screen Shot 2017-04-30 at 20.50.14.png" src="images/image1.png" style="width: 393.00px; height: 261.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c9 c12">Fig 4.2.1</span><span class="c10 c9">&nbsp;Baseline LSTM Confusion Matrix for the test set, resulting in 70% accuracy and a total score of 40.1. The rows indicate the ground truth labels, and the columns are the model predicted labels. </span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 410.00px; height: 243.50px;"><img alt="Screen Shot 2017-04-30 at 20.52.46.png" src="images/image9.png" style="width: 410.00px; height: 243.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c9 c12">Fig 4.2.2</span><span class="c10 c9">&nbsp;Bidirectional Conditioned Global Attention LSTM Confusion Matrix for the test set, resulting in 54% accuracy and a total score of 42.8. The rows indicate the ground truth labels, and the columns are the model predicted labels. </span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c11">5. Conclusions and next steps</span></p><p class="c1"><span class="c7"></span></p><p class="c6"><span class="c10 c9">&nbsp; &nbsp; We were able to achieve similar results and in some cases beat the baseline model using word2vec embeddings to represent words in the texts and training classifiers using various SVD and data manipulation to feed into ensemble methods. The best performing of these involved SVM classifiers, particularly with polynomial kernel, where on a 60/40 train/test split we were able to obtain a score of 79.2% compared to the baseline of 76.7%.</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">Unfortunately such accuracies were not able to be reproduced with the bidirectional LSTM, although certain interesting properties could be determined from the resulting confusion matrices.</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c6"><span class="c10 c9">Further work will involve continuing to develop the SVD models to feed into SVM classifiers, including possibly utilising higher order SVD (tensor-SVD) methods (some preliminary work can be found on our github link [0]) as well as look into some more innovative methods to penalise non-linear kernel classifiers. Furthermore, increased efforts will be made towards tuning the bidirectional LSTM to attempt to obtain higher accuracies before submission to the competition.</span></p><p class="c1"><span class="c10 c9"></span></p><p class="c4 c8 c15"><span class="c10 c9"></span></p><p class="c6"><span class="c12">6</span><span class="c17 c12">. References</span></p><p class="c4 c8 c15"><span class="c10 c9"></span></p><p class="c4 c8"><span class="c9">[0] Our code can be found in the following repository on github:</span><span class="c2"><a class="c19" href="https://www.google.com/url?q=https://github.com/goddenrich/AML&amp;sa=D&amp;ust=1493614662792000&amp;usg=AFQjCNEwQf7aiFJ2CdzxMXoz1NxNexgLLA">github.com/goddenrich/AML</a></span><span class="c10 c9">&nbsp;</span></p><p class="c4 c8 c15"><span class="c10 c9"></span></p><p class="c4 c8"><span class="c9">[1] D. Pomerleau and D. Rao, &ldquo;Fake news challenge.&rdquo;</span><span class="c2"><a class="c19" href="https://www.google.com/url?q=http://www.fakenewschallenge.org/&amp;sa=D&amp;ust=1493614662794000&amp;usg=AFQjCNFEYa04ypMsZx5b_xQehOrLi6OJiA">www.fakenewschallenge.org/</a></span><span class="c9">&nbsp;GitHub </span><span class="c2"><a class="c19" href="https://www.google.com/url?q=https://github.com/FakeNewsChallenge&amp;sa=D&amp;ust=1493614662795000&amp;usg=AFQjCNEbDzNSs6cJ-6f0kZSgALrTGB8Lrg">github.com/FakeNewsChallenge</a></span><span class="c10 c9">&nbsp;</span></p><p class="c4 c8"><span class="c9 c17">[</span><span class="c9">2</span><span class="c9 c17">] Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean Efficient Estimation of Word Representations in Vector Space arXiv:1301.3781 [cs.CL] URL</span><span class="c9">&nbsp;</span><span class="c2 c17"><a class="c19" href="https://www.google.com/url?q=https://arxiv.org/pdf/1301.3781.pdf&amp;sa=D&amp;ust=1493614662797000&amp;usg=AFQjCNHt8lGMhGJiiPu8PgdtT876SqBYmA">arxiv.org/pdf/1301.3781.pdf</a></span><span class="c9 c17">&nbsp;</span></p><p class="c4 c8"><span class="c9">[3] </span><span class="c2"><a class="c19" href="https://www.google.com/url?q=https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM&amp;sa=D&amp;ust=1493614662799000&amp;usg=AFQjCNGRPtUS0mVyl-8jdHzO-U-NOtySzg">drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM</a></span><span class="c10 c9">&nbsp;</span></p><p class="c4 c8"><span class="c9">[4] Prashanth Vijayaraghavan, Ivan Sysoev, Soroush Vosoughi and Deb Roy DeepStance at SemEval-2016 Task 6: Detecting Stance in Tweets Using Character and Word-Level CNNs arXiv:1606.05694 [cs.CL] </span><span class="c2"><a class="c19" href="https://www.google.com/url?q=https://arxiv.org/abs/1606.05694&amp;sa=D&amp;ust=1493614662800000&amp;usg=AFQjCNH3Fc0x5JpmnKJMrJOZRcaMJDo44w">arxiv.org/abs/1606.05694</a></span><span class="c10 c9">&nbsp;</span></p><p class="c4 c8"><span class="c9">[5] Mrowca, D., Wang, E., Kosson, A. Stance Detection for Fake News Identification. URL &nbsp;</span><span class="c2"><a class="c19" href="https://www.google.com/url?q=http://web.stanford.edu/class/cs224n/reports/2760496.pdf&amp;sa=D&amp;ust=1493614662802000&amp;usg=AFQjCNFjJEZkwRpnrj-WElSFt5_eYrxWvA">web.stanford.edu/class/cs224n/reports/2760496.pdf</a></span><span class="c10 c9">&nbsp;</span></p><p class="c4 c8"><span class="c9">[6] &nbsp;Liu Q. et al. Feature selection for support vector machines with RBF kernel. Artificial Intelligence Review 36(2):99-115 August 2011. URL &nbsp;</span><span class="c2"><a class="c19" href="https://www.google.com/url?q=https://www.researchgate.net/publication/220637867_Feature_selection_for_support_vector_machines_with_RBF_kernel&amp;sa=D&amp;ust=1493614662804000&amp;usg=AFQjCNFt-43J036KX2oHh6PQ5OAS8Af9Fg">www.researchgate.net/publication/220637867_Feature_selection_for_support_vector_machines_with_RBF_kernel</a></span><span class="c10 c9">&nbsp;</span></p><p class="c4 c8"><span class="c9">[7] </span><span class="c2"><a class="c19" href="https://www.google.com/url?q=http://www.paddlepaddle.org/doc/demo/sentiment_analysis/sentiment_analysis.html&amp;sa=D&amp;ust=1493614662805000&amp;usg=AFQjCNHOoXjOovH_SZnzLkrIDULcvDGQNw">www.paddlepaddle.org/doc/demo/sentiment_analysis/sentiment_analysis.html</a></span><span class="c10 c9">&nbsp;</span></p><p class="c4 c8"><span class="c9">[8] Tim Rockt&auml;schel, Edward Grefenstette, Karl Moritz Hermann, Tom&aacute;&scaron; Ko&#269;isk&yacute;, Phil Blunsom Reasoning about Entailment with Neural Attention arXiv:1509.06664 [cs.CL] </span><span class="c2"><a class="c19" href="https://www.google.com/url?q=https://arxiv.org/abs/1509.06664&amp;sa=D&amp;ust=1493614662807000&amp;usg=AFQjCNEskWG-80InKjVHZX7e4uPm3Oi_Wg">arxiv.org/abs/1509.06664</a></span><span class="c9">&nbsp;</span></p></body></html>